# Model parameters
img_size: [648, 712]  # [height, width]
patch_size: 16
in_channels: 30 # 30 since the mask is not evaluated in transformer
embed_dim: 768 # Must be divisible by num_heads (12) and also divisible by patch_size (8)
depth: 12
num_heads: 12
mlp_ratio: 4.0
dropout: 0.0

# Training parameters
batch_size: 48
num_epochs: 1000
learning_rate: 0.0001
weight_decay: 0.05
num_workers: 10
resume_from: null # "checkpoints/epoch_100.pth"
early_stopping_patience: 150  # Number of epochs to wait before early stopping
early_stopping_min_delta: 0.000001  # Minimum change in validation loss to qualify as an improvement

# Optimizer parameters
optimizer: "adamw"  # Options: "adam", "adamw"
adam_beta1: 0.9
adam_beta2: 0.999
adam_epsilon: 1.0e-8
num_warmup_steps: 100

# Dataset parameters
previous_days: 7
dataset_type: "gradient"  # Options: "regular", "extended", "gradient"

# Data paths
train_data_dir: "/unity/f1/ozavala/OUTPUTS/HR_SSH_from_Chlora/training_data"
val_data_dir: "/unity/f1/ozavala/OUTPUTS/HR_SSH_from_Chlora/training_data"

# Multi-GPU settings
adjust_batch_size: true
adjust_learning_rate: true

# Logging and saving
log_dir: "/unity/g2/jvelasco/ai_outs/task21_set1/training"  # Base directory for all experiments
run_name: "vit_nograd"  # This will create a folder inside log_dir with this name
# Subdirectories for logs and checkpoints will be automatically created inside run_name folder
save_dir: "/unity/g2/jvelasco/ai_outs/task21_set1/training" 